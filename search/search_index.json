{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Landing page","text":""},{"location":"Contribution%20Guidelines/CONTRIBUTING/","title":"Contributing to CuMind","text":"<p>Welcome to the Carleton University AI Society (CUAIS) CuMind project! Every function has comprehensive docstrings with everything you need to implement it.</p>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#development","title":"Development","text":"<p>The docstrings contain ALL the information you need! Each function includes:</p> <ul> <li>Args: What parameters to expect</li> <li>Returns: What to return and in what format</li> <li>Implementation: Step-by-step guidance on what to build</li> <li>Developer: Your name goes here when you implement it</li> <li>Branch: Branch name for this feature</li> </ul> <p>For example:</p> <pre><code>def select_action(self, observation: np.ndarray, training: bool = True) -&gt; int:\n  \"\"\"Select action using MCTS search from current observation.\n\n  Args:\n    observation: Current game state observation\n    training: If True, sample from action probabilities; if False, take best action\n\n  Returns:\n    Selected action index\n\n  Implementation:\n    - Convert observation to tensor and run network.initial_inference()\n    - Use MCTS to search and get action probabilities\n    - Sample action if training, else take argmax\n  \"\"\"\n  # Branch: feature/mcts-action-selection\n  raise NotImplementedError(\"select_action needs to be implemented\")\n</code></pre> <p>\ud83d\udca1 Pro Tip: Read documentation (mostly PyTorch), do your own research, use tools like ChatGPT and Copilot to help if you're struggling. Do not push anything you aren't sure about. Questions in the coders-corner channel are always welcomed!</p>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#branch-naming","title":"Branch Naming","text":"<p>Use these patterns for branch names:</p> <ul> <li><code>feature/description</code> \u2014 New features</li> <li><code>internal/description</code> \u2014 Refactoring or internal changes</li> <li><code>bugfix/description</code> \u2014 Bug fixes</li> <li><code>dev</code> \u2014 Development branch (anyone can push, but force-push is not allowed)</li> <li><code>master</code> \u2014 Protected branch (read-only)</li> </ul> <p>Examples:</p> <ul> <li><code>feature/add-atari-support</code></li> <li><code>internal/refactor-mcts</code></li> <li><code>bugfix/fix-reward-scaling</code></li> <li><code>my-feature</code> (not allowed)</li> <li><code>fix-bug</code> (not allowed)</li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#development-workflow","title":"Development Workflow","text":"<p>Follow these logical steps to contribute effectively:</p>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#1-set-up-your-environment","title":"1. Set Up Your Environment","text":"<ul> <li>Clone the repository using SSH (recommended) or HTTPS:</li> </ul> <pre><code># SSH (recommended)\ngit clone git@github.com:carletonai/cumind.git\n\n# HTTPS (requires personal access token to push)\ngit clone https://github.com/carletonai/cumind.git\n\ncd cumind\nuv sync\n</code></pre> <ul> <li>See:</li> <li>Managing your personal access tokens</li> <li>Connecting to GitHub with SSH</li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#2-sync-your-local-repository","title":"2. Sync Your Local Repository","text":"<p>Before starting any work, make sure your local repository is up to date:</p> <pre><code>git fetch\ngit pull dev\n</code></pre>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#3-find-a-function-to-implement","title":"3. Find a Function to Implement","text":"<ul> <li>Search for unimplemented functions:   <pre><code>grep -r \"NotImplementedError\" src/ --include=\"*.py\"\n</code></pre></li> <li>Review available branches for ongoing work:   <pre><code>git branch -a\ngit branch -a | grep \"feature/\"\n</code></pre></li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#4-finding-using-branches","title":"4. Finding &amp; Using Branches","text":"<ul> <li>Check for existing feature branches: <pre><code>git branch -a | grep \"feature/mcts-action\"\n</code></pre>   If a relevant branch exists, check it out and pull the latest changes:   <pre><code>git checkout feature/mcts-action-selection\ngit pull origin feature/mcts-action-selection\n</code></pre></li> <li>Create a new branch if needed: <pre><code>git checkout -b feature/describe-your-feature\n</code></pre></li> <li>Push your branch to remote: <pre><code>git push origin feature/describe-your-feature\n</code></pre></li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#5-implement-your-function","title":"5. Implement Your Function","text":"<ol> <li>Read the docstring for detailed instructions.</li> <li>Replace the <code>NotImplementedError</code> with your implementation.</li> <li>Update the developer field in the docstring</li> <li>Keep your code clean and simple.</li> </ol>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#6-update-corresponding-tests","title":"6. Update Corresponding Tests","text":"<ul> <li>For every function you implement, update its corresponding test.</li> <li>Example: If you implement <code>agent.py::select_action()</code>, also implement   <code>test_agent.py::test_select_action_training_mode()</code>.</li> <li>Find the relevant test:   <pre><code>find tests/ -name \"*.py\" -exec grep -l \"test_select_action\" {} \\;\n</code></pre></li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#7-test-validate","title":"7. Test &amp; Validate","text":"<ul> <li>Run your specific test:   <pre><code>uv run pytest tests/test_agent.py::TestAgent::test_select_action_training_mode -v\n</code></pre></li> <li>Run all tests:   <pre><code>uv run pytest tests/ -v\n</code></pre></li> <li>Run code quality checks:   <pre><code>uv run ruff check .\nuv run ruff format .\nuv run mypy src/\n</code></pre></li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#git-workflow","title":"Git Workflow","text":""},{"location":"Contribution%20Guidelines/CONTRIBUTING/#committing-your-work","title":"Committing Your Work","text":"<pre><code># Stage your changes\ngit add src/cumind/agent.py tests/test_agent.py\n\n# Commit with an informative message (can be goofy but informative!)\ngit commit -m \"feat: implement MCTS action selection\n\nAdded select_action method with exploration noise and visit count sampling.\nAlso implemented corresponding test with mock MCTS behavior.\"\n\n# Push your branch\ngit push origin feature/mcts-action-selection\n</code></pre> <p>Commit Message Tips:</p> <ul> <li>Be informative about what you implemented</li> <li>Mention if you also updated tests</li> <li>Goofy/fun messages are fine as long as they're clear!</li> <li>Some conventional commits are: <code>feat:</code>, <code>fix:</code>, <code>test:</code>, <code>docs:</code></li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#merging-others-work","title":"Merging Others' Work","text":"<p>If someone else worked on your branch:</p> <pre><code># Get the latest changes\ngit fetch origin\n\n# Merge their work into yours\ngit merge origin/feature/mcts-action-selection\n\n# Or rebase if you prefer clean history\ngit rebase origin/feature/mcts-action-selection\n</code></pre>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>Note: You only need a Pull Request (PR) for major changes to the <code>master</code> branch. For most work, push directly to <code>dev</code> after passing all checks.</p> <p>To open a PR to <code>master</code>:</p> <ol> <li>Push your branch to GitHub.</li> <li> <p>Open a PR targeting <code>master</code>.</p> </li> <li> <p>List the functions you implemented.</p> </li> <li>Briefly describe your approach and any challenges.</li> <li> <p>Link related issues if needed.</p> </li> <li> <p>Request a review from maintainers.</p> </li> <li>Make sure all checks pass (<code>pytest</code> and <code>mypy</code>).</li> </ol> <p>Else:</p> <ul> <li>Push to <code>dev</code> for regular work (tests/checks must pass).</li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#git-basics","title":"Git Basics","text":"<p>New to Git? Check out these resources:</p> <ul> <li>Git Handbook -   Official GitHub guide</li> <li>Learn Git Branching - Interactive   tutorial</li> <li>Oh Shit Git - Common problems and solutions</li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#code-style","title":"Code Style","text":"<p>Keep it simple and elegant!</p> <ul> <li>Follow the docstring guidance - it tells you exactly what to implement</li> <li>Use type hints everywhere (already provided)</li> <li>Keep functions focused - one responsibility per function</li> <li>Clean, readable code over clever code</li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#testing-philosophy","title":"Testing Philosophy","text":"<p>Every function implementation should update its corresponding test!</p>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#test-structure","title":"Test Structure","text":"<pre><code>src/cumind/agent.py     \u2192  tests/test_agent.py\nsrc/cumind/mcts.py      \u2192  tests/test_mcts.py\nsrc/cumind/network.py   \u2192  tests/test_network.py\n</code></pre>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#test-implementation","title":"Test Implementation","text":"<ul> <li>Replace <code>pass</code> with actual test logic</li> <li>Follow the test docstring guidance</li> <li>Test both success and failure cases</li> <li>Use descriptive assertions</li> </ul> <p>Example:</p> <pre><code>def test_select_action_training_mode(self):\n    \"\"\"Test action selection in training mode.\"\"\"\n    # Implementation: Test with mock MCTS, verify exploration\n    agent = Agent(config)\n    action = agent.select_action(observation, training=True)\n    assert isinstance(action, int)\n    assert 0 &lt;= action &lt; config.action_space_size\n</code></pre>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#project-structure","title":"Project Structure","text":"<pre><code>src/cumind/\n\u251c\u2500\u2500 agent.py      # CuMind agent with MCTS integration\n\u251c\u2500\u2500 mcts.py       # Monte Carlo Tree Search implementation\n\u251c\u2500\u2500 network.py    # Neural networks (representation, dynamics, prediction)\n\u2514\u2500\u2500 config.py     # Configuration and hyperparameters\n\ntests/\n\u251c\u2500\u2500 test_agent.py    # Agent functionality tests\n\u251c\u2500\u2500 test_mcts.py     # MCTS algorithm tests\n\u251c\u2500\u2500 test_network.py  # Neural network component tests\n\u2514\u2500\u2500 test_cumind.py   # Integration tests\n</code></pre>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#available-functions-to-implement","title":"Available Functions to Implement","text":"<p>Quick way to see what needs work:</p> <pre><code># See all unimplemented functions\ngrep -r \"NotImplementedError\" src/ --include=\"*.py\"\n\n# See all available remote branches\ngit branch -r\n</code></pre> <p>Popular starting points:</p> <ul> <li><code>feature/agent-initialization</code> - Set up the CuMind agent</li> <li><code>feature/mcts-action-selection</code> - MCTS-based action selection</li> <li><code>feature/vector-encoder</code> - Neural network for 1D observations (Classic   Control)</li> <li><code>feature/conv-encoder</code> - Neural network for 3D observations (Atari)</li> <li><code>feature/residual-block</code> - Building block for neural networks</li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#community-guidelines","title":"Community Guidelines","text":"<ul> <li>Be respectful and inclusive</li> <li>Help each other in discussions and reviews</li> <li>Share knowledge - document tricky parts</li> <li>Ask questions if docstrings aren't clear enough</li> <li>Celebrate progress - every implementation helps!</li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#tips-for-success","title":"Tips for Success","text":"<ul> <li>Start small: Pick an easy function to get comfortable.</li> <li>Explore related code: Read similar functions to see how things fit   together.</li> <li>Test as you go: Run tests early and often\u2014don\u2019t wait until the end.</li> <li>Use examples: Check out examples for inspiration. (TODO)</li> <li>Ask for help: If you\u2019re stuck, open an issue or chat on Discord.</li> </ul> <p>Feeling stuck? That\u2019s normal!</p> <ul> <li>Re-read the docstring\u2014often the answer is there.</li> <li>Look at how similar functions are implemented.</li> <li>Search existing issues or discussions for hints.</li> <li>If you need to, open a new issue. Include:</li> <li>The function you\u2019re working on</li> <li>What you\u2019ve tried so far</li> <li>Where you\u2019re stuck</li> <li>Any relevant code snippets</li> <li>If you\u2019re blocked for more than a day, reach out to maintainers on Discord,   we\u2019re here to help!</li> </ul> <p>Common stumbling blocks:</p> <ul> <li>PyTorch confusion? Ask AI or the community for explanations.</li> <li>Failing tests? Double-check the test docstring for what\u2019s expected.</li> <li>Branch conflicts? See the Git Basics section above.</li> <li>Unclear requirements? Open an issue to clarify or improve the docstring.</li> </ul>"},{"location":"Contribution%20Guidelines/CONTRIBUTING/#ready-to-contribute","title":"Ready to Contribute?","text":"<ol> <li>Find a function you want to implement</li> <li>Check if someone started it (<code>git branch -a | grep feature/...</code>)</li> <li>Create/checkout the branch</li> <li>Read the docstring and implement it</li> <li>Update the corresponding test</li> <li>Test your changes</li> <li>Commit and push</li> </ol> <p>Welcome to the CUAIS community!</p> <p>Every function you implement brings us closer to a complete CuMind implementation. Thank you for contributing!</p>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/","title":"GPU Performance Optimization Guide for CuMind","text":"<p>This document provides comprehensive GPU performance optimization tips for the CuMind project, based on JAX best practices and analysis of the current codebase.</p>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#quick-start","title":"Quick Start","text":""},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#essential-gpu-optimizations","title":"Essential GPU Optimizations","text":"<ol> <li>Enable XLA optimizations by setting environment variables before running:</li> </ol> <pre><code>export XLA_FLAGS='--xla_gpu_triton_gemm_any=True --xla_gpu_enable_latency_hiding_scheduler=true'\nexport JAX_ENABLE_PGLE=true\nexport JAX_PGLE_PROFILING_RUNS=3\n</code></pre> <ol> <li>Use mixed precision by configuring dtypes in <code>configuration.json</code>:</li> </ol> <pre><code>\"Data Types\": {\n    \"model_dtype\": \"bfloat16\",\n    \"action_dtype\": \"int32\",\n    \"target_dtype\": \"float32\"\n}\n</code></pre> <ol> <li>Run with one process per GPU:</li> </ol> <pre><code># For single GPU\npython -m cumind train\n\n# For multi-GPU (example with 4 GPUs)\npython -m cumind train --num-devices 4\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#environment-configuration","title":"Environment Configuration","text":""},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#xla-compiler-flags","title":"XLA Compiler Flags","text":"<p>Set these environment variables for optimal GPU performance:</p> <pre><code>import os\n\n# Basic XLA optimizations\nos.environ['XLA_FLAGS'] = (\n    '--xla_gpu_triton_gemm_any=True '  # Enable Triton GEMM kernels\n    '--xla_gpu_enable_latency_hiding_scheduler=true '  # Better scheduling\n    '--xla_gpu_enable_async_collectives=true '  # Async communication\n)\n\n# Profile-Guided Latency Estimation\nos.environ['JAX_ENABLE_PGLE'] = 'true'\nos.environ['JAX_PGLE_PROFILING_RUNS'] = '3'  # Number of profiling runs\n\n# NCCL communication optimization (for multi-GPU)\nos.environ.update({\n    \"NCCL_LL128_BUFFSIZE\": \"-2\",\n    \"NCCL_LL_BUFFSIZE\": \"-2\",\n    \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n})\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#pipeline-parallelism-advanced","title":"Pipeline Parallelism (Advanced)","text":"<p>For large models with pipeline parallelism:</p> <pre><code>export XLA_FLAGS=\"${XLA_FLAGS} --xla_gpu_enable_command_buffer='' --xla_disable_hlo_passes=collective-permute-motion --xla_gpu_experimental_pipeline_parallelism_opt_level=PIPELINE_PARALLELISM_OPT_LEVEL_ENABLE\"\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#code-optimizations","title":"Code Optimizations","text":""},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#1-jit-compilation","title":"1. JIT Compilation","text":"<p>Add JIT compilation to performance-critical functions:</p> <pre><code># In agent.py\nimport jax\n\nclass Agent:\n    @partial(jax.jit, static_argnames=['training'])\n    def select_action(self, observation: np.ndarray, training: bool = True) -&gt; int:\n        # ... existing code ...\n\n# In trainer.py\nclass Trainer:\n    @jax.jit\n    def _loss_fn(self, params, target_params, batch):\n        # ... existing code ...\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#2-mixed-precision-training","title":"2. Mixed Precision Training","text":"<p>Use bfloat16 for faster computation on modern GPUs (A100, V100):</p> <pre><code># In network.py\ndef __init__(self, ..., model_dtype: str = \"bfloat16\"):\n    self.dtype = get_dtype(model_dtype)\n\n    # Use mixed precision in layers\n    self.dense = nnx.Dense(\n        in_features,\n        out_features,\n        dtype=self.dtype,  # Computation in bfloat16\n        param_dtype=jnp.float32  # Parameters in float32\n    )\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#3-batch-processing-optimization","title":"3. Batch Processing Optimization","text":"<p>Utilize the existing <code>batched_apply</code> function for memory-efficient processing:</p> <pre><code># In trainer.py\nfrom ..utils.jax_utils import batched_apply\n\n# Process large batches in chunks\npredictions = batched_apply(\n    self.network.initial_inference,\n    observations,\n    batch_size=32  # Process 32 samples at a time\n)\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#4-device-placement","title":"4. Device Placement","text":"<p>Explicitly place computations on GPU:</p> <pre><code># In agent.py\nimport jax\n\n# Check available devices\nprint(f\"Available devices: {jax.devices()}\")\n\n# Place arrays on GPU\nobservation_gpu = jax.device_put(observation, jax.devices('gpu')[0])\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#implementation-examples","title":"Implementation Examples","text":""},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#example-1-optimized-training-step","title":"Example 1: Optimized Training Step","text":"<pre><code># In trainer.py\nimport functools\n\nclass Trainer:\n    def __init__(self, config: Config):\n        # ... existing code ...\n\n        # JIT compile the training step\n        self._train_step_jit = jax.jit(self._train_step)\n\n    @functools.partial(jax.jit, donate_argnums=(0, 1))\n    def _train_step(self, params, opt_state, batch):\n        \"\"\"JIT-compiled training step with donated buffers.\"\"\"\n        grads = jax.grad(self._loss_fn)(params, self.target_params, batch)\n        updates, opt_state = self.optimizer.update(grads, opt_state, params)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#example-2-mixed-precision-network","title":"Example 2: Mixed Precision Network","text":"<pre><code># In config.py\n@chex.dataclass\nclass Config:\n    # ... existing fields ...\n\n    # GPU optimization settings\n    use_mixed_precision: bool = True\n    jit_compile: bool = True\n    xla_flags: str = \"--xla_gpu_triton_gemm_any=True\"\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#example-3-multi-gpu-training","title":"Example 3: Multi-GPU Training","text":"<pre><code># In runner.py\nimport jax\nimport jax.numpy as jnp\nfrom jax import pmap\n\nclass DistributedRunner:\n    def __init__(self, config: Config):\n        self.num_devices = jax.device_count()\n        print(f\"Using {self.num_devices} GPUs\")\n\n        # Replicate model across devices\n        self.train_step = pmap(self._train_step, axis_name='devices')\n\n    def train_epoch(self, data):\n        # Shard data across devices\n        data_per_device = data.reshape(self.num_devices, -1, *data.shape[1:])\n\n        # Parallel training step\n        new_params = self.train_step(self.params, data_per_device)\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#benchmarking","title":"Benchmarking","text":""},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#performance-monitoring-script","title":"Performance Monitoring Script","text":"<p>Create <code>scripts/benchmark_gpu.py</code>:</p> <pre><code>import time\nimport jax\nimport jax.numpy as jnp\nfrom cumind import Agent, Config\n\ndef benchmark_inference(config: Config, num_runs: int = 1000):\n    \"\"\"Benchmark inference speed on GPU.\"\"\"\n    agent = Agent(config)\n    observation = jnp.ones(config.observation_shape)\n\n    # Warm-up\n    for _ in range(10):\n        _ = agent.select_action(observation, training=False)\n\n    # Benchmark\n    start_time = time.time()\n    for _ in range(num_runs):\n        _ = agent.select_action(observation, training=False)\n\n    # Force completion\n    jax.block_until_ready(agent.network.state)\n\n    elapsed = time.time() - start_time\n    print(f\"Inference speed: {num_runs / elapsed:.2f} steps/second\")\n\nif __name__ == \"__main__\":\n    # Test different configurations\n    configs = [\n        Config(model_dtype=\"float32\"),\n        Config(model_dtype=\"bfloat16\"),\n    ]\n\n    for config in configs:\n        print(f\"\\nTesting with dtype: {config.model_dtype}\")\n        benchmark_inference(config)\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li>Out of Memory (OOM) Errors</li> <li>Reduce batch size</li> <li>Use gradient accumulation</li> <li> <p>Enable memory optimization:      <code>os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'</code></p> </li> <li> <p>Slow Compilation</p> </li> <li>Cache compiled functions:      <code>jax.config.update('jax_persistent_cache_min_compile_time_secs', 1.0)</code></li> <li> <p>Reduce JIT compilation overhead by batching operations</p> </li> <li> <p>Mixed Precision Instability</p> </li> <li>Keep loss scaling in float32</li> <li>Use float32 for batch normalization statistics</li> <li> <p>Monitor for NaN/Inf values</p> </li> <li> <p>Multi-GPU Synchronization</p> </li> <li>Ensure all devices have same batch size</li> <li>Use <code>psum</code> for gradient aggregation</li> <li>Check NCCL environment variables</li> </ol>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#profiling-tools","title":"Profiling Tools","text":"<p>Use JAX profiler to identify bottlenecks:</p> <pre><code># Enable profiling\nimport jax.profiler\n\n# Profile a training step\nwith jax.profiler.trace(\"/tmp/jax-trace\"):\n    agent.train_step(batch)\n\n# View in TensorBoard\n# tensorboard --logdir=/tmp/jax-trace\n</code></pre>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#future-optimizations","title":"Future Optimizations","text":""},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#planned-improvements","title":"Planned Improvements","text":"<ol> <li>Automatic Mixed Precision (AMP)</li> <li>Implement dynamic loss scaling</li> <li> <p>Add automatic dtype conversion</p> </li> <li> <p>Fused Kernels</p> </li> <li>Combine operations for better memory bandwidth</li> <li> <p>Use XLA fusion hints</p> </li> <li> <p>Quantization</p> </li> <li>INT8 inference for deployment</li> <li> <p>Quantization-aware training</p> </li> <li> <p>Advanced Parallelism</p> </li> <li>Model parallelism for large networks</li> <li>Pipeline parallelism for sequential models</li> </ol>"},{"location":"Knowledge%20Base/GPU_PERFORMANCE_TIPS/#references","title":"References","text":"<ul> <li>JAX GPU Performance Tips</li> <li>JAX JIT Compilation</li> <li>Mixed Precision Training</li> <li>XLA Optimization Passes</li> </ul> <p>Last updated: 2025-07-04 Tested with: JAX 0.4.x, CUDA 12.x, CuMind v0.1.x</p>"},{"location":"Knowledge%20Base/test/","title":"test","text":""}]}